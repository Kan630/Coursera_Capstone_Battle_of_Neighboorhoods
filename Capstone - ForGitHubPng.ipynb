{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/a/ae/Tour_Eiffel_360_Panorama.jpg/1600px-Tour_Eiffel_360_Panorama.jpg'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align='center'><h1>PARIS, FRANCE - Hotels, locations & ratings</h1></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr><td>\n",
    "<img src='images/4square.png'></img>\n",
    "    </td><td>\n",
    "<img src='images/Paris_Data.png'></img>\n",
    "    </td><td>\n",
    "<img src='images/Google_API.jpg'></img>\n",
    "    </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import math\n",
    "import os\n",
    "\n",
    "#!conda install -c conda-forge folium --yes     \n",
    "import folium\n",
    "\n",
    "#!conda install -c conda-forge plotly --yes     \n",
    "#import plotly.express as px  # => for other chloropet maps\n",
    "\n",
    "#!conda install -c conda-forge geopy --yes\n",
    "import geopy.distance\n",
    "\n",
    "#!conda install -c conda-forge shapely --yes\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "#!conda install -c conda-forge selenium --yes\n",
    "from selenium import webdriver # for firefox png save\n",
    "\n",
    "from IPython.display import Markdown as md, HTML, IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell   ## FOR WATSON STUDIO\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "\n",
    "#from project_lib import Project\n",
    "#project = Project(project_id='ed04c54a-dc39-4bb6-af07-e54a55093bff', project_access_token='p-ed603ae246f1da06783604e333d6c9f0ed67a997')\n",
    "#pc = project.project_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "SQUARE_CLIENT_ID = 'VZPDH3T34ONPVG2DU3NEIIAZ011X25RF43O0DDKXVMJIK45F' # your Foursquare ID\n",
    "SQUARE_CLIENT_SECRET = 'MVS0NFTG1BPNJDCNJXO3R2MOCNTAMK4EDASTKU33POXJZ4GO' # your Foursquare Secret\n",
    "GOOGLE_API_KEY = 'AIzaSyDgibZm5ixDJv9g197Wpnhy9ZP6BArd8mc'\n",
    "\n",
    "VERSION = '20190909' # Foursquare API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP PARAMETERS\n",
    "LAT_PARIS = 48.858\n",
    "LON_PARIS = 2.3488\n",
    "ZOOM_START_PARIS = 12\n",
    "TILE_PARIS = 'Stamen Terrain'   # 'Stamen Terrain', 'Stamen Toner', 'cartodbpositron'\n",
    "sw0_lat=48.814\n",
    "ne0_lat=48.912\n",
    "sw0_lon=2.248\n",
    "ne0_lon=2.44\n",
    "\n",
    "Dic_Station_Coords = {'Gare Montparnasse' : [48.840840, 2.320062]\n",
    "                    ,'Gare Saint-Lazare' : [48.877148, 2.324804]\n",
    "                    ,'Gare du Nord' : [48.881284, 2.355424]\n",
    "                    ,'Gare de l\\'Est' : [48.877168, 2.359292]\n",
    "                    ,'Gare de Lyon' : [48.844495, 2.374082]\n",
    "                    }\n",
    "\n",
    "Dic_Yellow_Coords = {'Notre-Dame' : [48.858, 2.3488]\n",
    "                    ,'New Center' : [48.8646, 2.3214]\n",
    "                    }\n",
    "LAT_NewCenter = 48.8646\n",
    "LON_NewCenter =  2.3214\n",
    "\n",
    "colordict = {0:'red',1:'blue',2:'green',3:'yellow',4:'black'}\n",
    "\n",
    "# CLustering parameters\n",
    "kclusters_for_HostelVenues = 2\n",
    "kclusters_for_quartiers80 = 2\n",
    "num_top_venues = 5\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# DEBUG PARAMETER\n",
    "DO_DOWNLOAD_NEW_DATA = False\n",
    "DO_CREATE_MAPS = False\n",
    "KIND_OF_MAP_TO_DISPLAY = 'PNG'  #'PYTHON', 'HTML', 'PNG'\n",
    "\n",
    "# Threshold for discretization of rating : 0 or 1\n",
    "THRESHOLD_GOOGLE_RATING = 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My  functions\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Load and Save Dataframes\n",
    "# --------------------------------------------------------------------------------\n",
    "def mySave(df,filepath) :\n",
    "    df.to_csv(filepath)\n",
    "    #project.save_data(data=df.to_csv(index=False),file_name=filepath,overwrite=True)\n",
    "\n",
    "def myLoad(filepath) :\n",
    "    df = pd.read_csv(filepath)\n",
    "    #df = pd.read_csv(project.get_file(filepath))    \n",
    "    df = df.fillna('')\n",
    "    return(df)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# To exit cell properly\n",
    "# --------------------------------------------------------------------------------\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "    \n",
    "# --------------------------------------------------------------------------------\n",
    "# How to display pictures\n",
    "# --------------------------------------------------------------------------------\n",
    "def Display_MapPicture(zemap,zefile) :\n",
    "    if KIND_OF_MAP_TO_DISPLAY == 'PYTHON' : return(zemap)\n",
    "    elif KIND_OF_MAP_TO_DISPLAY == 'HTML' : return(IFrame(src=zefile+'.html', width=800, height=450))\n",
    "    elif KIND_OF_MAP_TO_DISPLAY == 'PNG' : display(md(\"![... Map not ready or non existent...](\" + zefile + \".png)\"))\n",
    "    else : print(\"ERROR - bad value for CONSTANT KIND_OF_MAP_TO_DISPLAY\")    \n",
    "\n",
    "def Proceed_with_map(zemap,zefile,DoShow = True) :\n",
    "    if DO_CREATE_MAPS : ConvertMapToPng(zemap, zefile ,2)\n",
    "    if DoShow : return(Display_MapPicture(zemap,zefile))\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# To export folium map to png image\n",
    "# --------------------------------------------------------------------------------\n",
    "def ConvertMapToPng(zeMap, zeFileName, LoadDelay) :\n",
    "    delay = LoadDelay\n",
    "\n",
    "    #Save the map as an HTML file\n",
    "    fn = zeFileName + '.html'\n",
    "    tmpurl = 'file://{path}/{mapfile}'.format(path=os.getcwd(),mapfile=fn)\n",
    "    zeMap.save(fn)\n",
    "    \n",
    "    # make a screenshot of the html and store in into a png\n",
    "    browser = webdriver.Firefox(executable_path = r'tools/geckodriver.exe') # You need Firefox installed...\n",
    "    browser.get(tmpurl)\n",
    "    time.sleep(delay) #Give the map tiles some time to load\n",
    "    browser.save_screenshot(zeFileName + '.png')\n",
    "    browser.quit()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # List all hostels in Paris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 4square for this task.  \n",
    "Sadly, there is no <b>offset</b> parameter for 4square 'search' API, this parameter exists only for 'explore' API   \n",
    "(<i>Offset parameter is used to get the next 50 results from the api call</i>)  \n",
    "But on the other way, the parameter <b>categoryId</b> is only available for the 'search' API   \n",
    "So the workaround to get all hotels of Paris is to divide Paris geographically in small blocks returning each less than 50 hotels and to use the search API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Polygons (rectangles) to cover the City region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many boxes for our Paris region\n",
    "nbVertical=10\n",
    "nbHorizontal=10\n",
    "\n",
    "# Compute the surface of a box\n",
    "#def measure(lat1, lon1, lat2, lon2) :  # generally used geo measurement function\n",
    "#    R = 6378.137; # Radius of earth in KM\n",
    "#    dLat = lat2 * math.pi / 180 - lat1 * math.pi / 180;\n",
    "#    dLon = lon2 * math.pi / 180 - lon1 * math.pi / 180;\n",
    "#    a = math.sin(dLat/2) * math.sin(dLat/2) + math.cos(lat1 * math.pi / 180) * math.cos(lat2 * math.pi / 180) * math.sin(dLon/2) * math.sin(dLon/2);\n",
    "#    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "#    d = R * c;\n",
    "#    return d * 1000; # meters\n",
    "#coords1 = (row.lat,row.lon)\n",
    "#coords2 = (myList[index][5],myList[index][6])\n",
    "#zeDist = geopy.distance.distance(coords1, coords2).km # distance between 4square & google positions\n",
    "\n",
    "\n",
    "# Create a geo json file that divide Paris in small boxes (for easy viewing)\n",
    "# Create a list of (sw,ne) coordinates to pass to 4square API\n",
    "def GetPolygonBoxesGeoJsonFile(sw_lat, sw_lon, ne_lat, ne_lon ,nbVertical, nbHorizontal) :\n",
    "\n",
    "    global Area_Box\n",
    "    geojsonfileStart = '{\"type\": \"FeatureCollection\",\"features\": ['\n",
    "    geojsonfileEnd = ']}'\n",
    "    geojsonSectionStart = '{\"type\": \"Feature\", \"geometry\":{\"type\": \"Polygon\",\"coordinates\": ['\n",
    "    geojsonSectionEnd = ']},\"properties\": {\"objectid\": 1}}' #]},\"properties\": {\"objectid\": 1}}\n",
    "    myFileContent=''\n",
    "    myFileContent = geojsonfileStart\n",
    "    extend_vertical = ne_lat-sw_lat\n",
    "    extend_horizontal = ne_lon-sw_lon\n",
    "    pas_vertical = extend_vertical/nbVertical\n",
    "    pas_horizontal = extend_horizontal/nbHorizontal\n",
    "    List_lat = np.around(np.arange(sw0_lat,ne0_lat,pas_vertical),5)\n",
    "    List_lon = np.around(np.arange(sw0_lon,ne0_lon,pas_horizontal),5)\n",
    "\n",
    "    # Compute Box Surface\n",
    "    #long = measure(List_lat[0],List_lon[0],List_lat[1],List_lon[0])  # vertical\n",
    "    #larg = measure(List_lat[0],List_lon[0],List_lat[0],List_lon[1])  # horizontal\n",
    "    long = geopy.distance.distance([List_lat[0],List_lon[0]],[List_lat[1],List_lon[0]]).m  # vertical\n",
    "    larg = geopy.distance.distance([List_lat[0],List_lon[0]],[List_lat[0],List_lon[1]]).m  # horizontal\n",
    "    Area_Box = long * larg\n",
    "\n",
    "    # Create the polygons\n",
    "    New4squareCoordDic = {}\n",
    "    Starting = True\n",
    "    for j in range(len(List_lat)-1) :\n",
    "        for i in range(len(List_lon)-1) :\n",
    "            myFileContent += geojsonSectionStart\n",
    "            zeID = str(i) + '_' + str(j)\n",
    "            \n",
    "            # list of sw & ne corners, to pass to 4square API\n",
    "            New4squareCoordDic[zeID] = { 'sw' : str(List_lat[i]) + ',' + str(List_lon[j]) \n",
    "                                        , 'ne' : str(List_lat[i+1]) + ',' + str(List_lon[j+1]) }\n",
    "            \n",
    "            # 5 points to create a geo json polygon, for the map\n",
    "            NewPolygon = '[[' + str(List_lon[i]) + ',' + str(List_lat[j]) + ']' + \\\n",
    "                 ',[' + str(List_lon[i+1]) + ',' + str(List_lat[j]) + ']' + \\\n",
    "                 ',[' + str(List_lon[i+1]) + ',' + str(List_lat[j+1]) + ']' + \\\n",
    "                 ',[' + str(List_lon[i]) + ',' + str(List_lat[j+1]) + ']' + \\\n",
    "                 ',[' + str(List_lon[i]) + ',' + str(List_lat[j]) + ']]'\n",
    "            \n",
    "            #print('{}-{}'.format(i,j))\n",
    "            #print(NewPolygon)\n",
    "            if not Starting :\n",
    "                myFileContent += ','\n",
    "                Starting=False\n",
    "            myFileContent += NewPolygon\n",
    "            geojsonSectionEnd = ']},\"properties\": {\"objectid\": \"' + zeID + '\"}},' #]},\"properties\": {\"objectid\": 1}}\n",
    "            myFileContent += geojsonSectionEnd\n",
    "    \n",
    "    # Finish writing geojson file\n",
    "    myFileContent = myFileContent[:-1]\n",
    "    myFileContent += geojsonfileEnd\n",
    "    return(myFileContent,New4squareCoordDic)\n",
    "\n",
    "\n",
    "# MAIN\n",
    "myGeoJsonFile,my4squareDic = GetPolygonBoxesGeoJsonFile(sw0_lat, sw0_lon, ne0_lat, ne0_lon ,nbVertical, nbHorizontal)\n",
    "with open('datasaved/Paris_Boxes.json', 'w') as f: {f.write(myGeoJsonFile)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the polygon mapped region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "![... Map not ready or non existent...](datasaved/Map_Paris_FirstBoxes.png)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiate the map\n",
    "mymap = folium.Map(location=[LAT_PARIS, LON_PARIS], zoom_start=ZOOM_START_PARIS, tiles=TILE_PARIS) #tiles='Stamen Terrain', 'Stamen Toner'\n",
    "\n",
    "# add the json boundaries\n",
    "with open('datasaved/Paris_Boxes.json', 'r') as output: boundaries_box = json.load(output)\n",
    "folium.GeoJson(boundaries_box,name='geojson').add_to(mymap)\n",
    "\n",
    "Proceed_with_map(mymap,'datasaved/Map_Paris_FirstBoxes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Api Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the created polygon (rectangles) and ask 4square to list all hotel in this rectangle.  \n",
    "If the API returns 50 hotel in one rectangle, that means that they may well be over 50 in that rectangle,   \n",
    "and so, we will divide the rectangle into 4 smaller rectangles and run 4 api calls.  \n",
    "This process is recursive and so, we can initially divide the city region into as many rectangles as we want, that doesn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA : \n",
    "    print('NOT SENDING API CALLS, LOADING SAVED DATA INSTEAD')\n",
    "    raise StopExecution\n",
    "\n",
    "#https://api.foursquare.com/v2/venues/search?intent=browse&client_id=VZPDH3T34ONPVG2DU3NEIIAZ011X25RF43O0DDKXVMJIK45F&client_secret=MVS0NFTG1BPNJDCNJXO3R2MOCNTAMK4EDASTKU33POXJZ4GO&v=20180605&categoryId=4bf58dd8d48988d1fa931735&sw=48.814,2.248&ne=48.8238,2.2672&limit=50\n",
    "categoryId='4bf58dd8d48988d1fa931735' # hotels\n",
    "intent='browse'\n",
    "version='20190905'\n",
    "limit=50\n",
    "\n",
    "myListPlace=[]\n",
    "df = pd.DataFrame()\n",
    "k=0\n",
    "nbBox = len(my4squareDic)\n",
    "\n",
    "# This function make a 4square 'venue/search' api call to return a list of venues of the specified categoryId\n",
    "def GetVenueSearchResults(sw,ne) :\n",
    "    url = 'https://api.foursquare.com/v2/venues/search?intent=browse&client_id={}&client_secret={}&v={}&categoryId={}&sw={}&ne={}&limit={}'.format(\n",
    "        SQUARE_CLIENT_ID, \n",
    "        SQUARE_CLIENT_SECRET, \n",
    "        VERSION, \n",
    "        categoryId,\n",
    "        sw,\n",
    "        ne,\n",
    "        limit)\n",
    "    results = requests.get(url).json()\n",
    "    try :\n",
    "        nbPlace = len(results['response']['venues'])\n",
    "    except KeyError :\n",
    "        print('ERROR')\n",
    "        print(url)\n",
    "        print(result)\n",
    "    return(results)\n",
    "\n",
    "# This function split a box (sw,ne) into 4 smaller boxes\n",
    "def SplitBox(sw,ne,keyDicPrefix) :\n",
    "    lat0 = sw.split(',')[0] \n",
    "    lat1 = ne.split(',')[0] \n",
    "    lat_m = float(lat0) + (float(lat1) - float(lat0))/2\n",
    "    lon0 = sw.split(',')[1] \n",
    "    lon1 = ne.split(',')[1] \n",
    "    lon_m = float(lon0) + (float(lon1) - float(lon0))/2\n",
    "    NewSquareDic = {}\n",
    "    NewSquareDic[keyDicPrefix + '_1'] = {'sw' : str(lat0) + ',' + str(lon0) , 'ne' : str(lat_m) + ',' + str(lon_m)}\n",
    "    NewSquareDic[keyDicPrefix + '_2'] = {'sw' : str(lat_m) + ',' + str(lon0) , 'ne' : str(lat1) + ',' + str(lon_m)}\n",
    "    NewSquareDic[keyDicPrefix + '_3'] = {'sw' : str(lat0) + ',' + str(lon_m) , 'ne' : str(lat_m) + ',' + str(lon1)}\n",
    "    NewSquareDic[keyDicPrefix + '_4'] = {'sw' : str(lat_m) + ',' + str(lon_m) , 'ne' : str(lat1) + ',' + str(lon1)}\n",
    "    return(NewSquareDic)\n",
    "\n",
    "# This is a recursive function that list the venues of a geographical box\n",
    "# It is recursive because it will keep calling itself on smaller geographical boxes if more than 50 results are returned\n",
    "def CountVenues(sw,ne,key) :\n",
    "    #print('{}  -  {}  -  {}'.format(key,sw,ne))\n",
    "    global myList\n",
    "    global zeTotal\n",
    "    results = GetVenueSearchResults(sw,ne) \n",
    "    nbVenuesReturned = len(results['response']['venues'])\n",
    "    if nbVenuesReturned > 49 :\n",
    "        # 50 venues returned. It's the max, let's divide the region into 4 lesser regions and do it again\n",
    "        #print('sector {} returns max nb of place. Recursive call en route'.format(key))\n",
    "        print('sector {} returns max nb of place. Recursive call en route'.format(key))\n",
    "        NewSquareDic = SplitBox(sw,ne,key)\n",
    "        for key2 in NewSquareDic :\n",
    "            sw = NewSquareDic[key2]['sw']\n",
    "            ne = NewSquareDic[key2]['ne']\n",
    "            newCount = CountVenues(sw,ne,key2) # <=== Recursive Call\n",
    "            \n",
    "    else :\n",
    "        # ok, less than 50 venues, let's parse the returned json file\n",
    "        print('{}  :  {} venues'.format(key,nbVenuesReturned))\n",
    "        zeTotal = zeTotal + nbVenuesReturned\n",
    "        for i in range (0,nbVenuesReturned) :\n",
    "            try : zeCP = results['response']['venues'][i]['location']['postalCode']\n",
    "            except KeyError : zeCP = ''\n",
    "            try : zeCity = results['response']['venues'][i]['location']['city']\n",
    "            except KeyError : zeCity = ''\n",
    "            try : zeFormattedAddress = results['response']['venues'][i]['location']['formattedAddress']\n",
    "            except KeyError : zeFormattedAddress = ''\n",
    "            myList.append([results['response']['venues'][i]['name'], \n",
    "                            results['response']['venues'][i]['id'], \n",
    "                            results['response']['venues'][i]['location']['lat'], \n",
    "                            results['response']['venues'][i]['location']['lng'], \n",
    "                            zeCP, \n",
    "                            zeCity, \n",
    "                            zeFormattedAddress, \n",
    "                            results['response']['venues'][i]['categories'][0]['name'],\n",
    "                            ])\n",
    "    return(zeTotal)\n",
    "\n",
    "\n",
    "\n",
    "# MAIN LOOP\n",
    "for key in my4squareDic :   # loop on the dictionnary of boxes\n",
    "    sw = my4squareDic[key]['sw']\n",
    "    ne = my4squareDic[key]['ne']\n",
    "    k=k+1\n",
    "    myList = []\n",
    "    zeTotal = 0\n",
    "    # api calls are made here in this function \n",
    "    nbPlace = CountVenues(sw,ne,key)\n",
    "    my4squareDic[key]['nb'] = nbPlace\n",
    "    #print('\\r', 'GeoBox {} n°{}/{} - {} venues found'.format(key,k,nbBox,nbPlace), end='                              ')\n",
    "    print('GeoBox {} n°{}/{} - {} venues found'.format(key,k,nbBox,nbPlace))\n",
    "    myDF = pd.DataFrame(myList)\n",
    "    df = df.append(myDF)    \n",
    "\n",
    "df.columns=['name','id','lat','lon','PostalCode','City','formattedAddress','type']\n",
    "df.reset_index(inplace=True)\n",
    "df = df.drop('index',axis=1)\n",
    "mySave(df,'datasaved\\All00.csv')\n",
    "#print('\\r', 'Ready...', end='                              ')\n",
    "print('Ready...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA :\n",
    "    df = pd.DataFrame()\n",
    "    df = myLoad('datasaved\\All00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nbHotel = df.shape[0]\n",
    "print('The DataFrame got {} hotels'.format(nbHotel))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's represent these hotels on a grid map of Paris "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : This must be optimized. It's slow. TODO !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile a geo json file of Paris, cut into 400 rectangular boxes\n",
    "myGeoJson = GetPolygonBoxesGeoJsonFile(sw0_lat, sw0_lon, ne0_lat, ne0_lon ,nbVertical=20, nbHorizontal=20)\n",
    "\n",
    "# Load this json file into a dictionnary\n",
    "boundaries_box = json.loads(myGeoJson[0])\n",
    "\n",
    "#  compute the area of one box\n",
    "coords1 = boundaries_box['features'][0]['geometry']['coordinates'][0][0]\n",
    "coords2 = boundaries_box['features'][0]['geometry']['coordinates'][0][1]\n",
    "L1 = geopy.distance.distance(coords1, coords2).km # distance between 4square & google positions\n",
    "coords1 = boundaries_box['features'][0]['geometry']['coordinates'][0][1]\n",
    "coords2 = boundaries_box['features'][0]['geometry']['coordinates'][0][2]\n",
    "L2 = geopy.distance.distance(coords1, coords2).km # distance between 4square & google positions\n",
    "Box_Area = L1*L2    \n",
    "\n",
    "# Assignate to each hotel its box id\n",
    "# Note : this way os searching is not optimized... Todo...\n",
    "zeList=[]\n",
    "for index, row in df.iterrows() :\n",
    "    found = False\n",
    "    point = Point(row['lon'],row['lat']) # create point\n",
    "    for i in range(len(boundaries_box['features'])) :\n",
    "        PolyId = boundaries_box['features'][i]['properties']['objectid']\n",
    "        PolyCoords = boundaries_box['features'][i]['geometry']['coordinates'][0]\n",
    "        polygon = Polygon((PolyCoords)) # create polygon\n",
    "        if polygon.contains(point) : # check if polygon contains point\n",
    "            zeList.append(PolyId)\n",
    "            found = True\n",
    "            break\n",
    "    if found == False :\n",
    "        zeList.append('')\n",
    "\n",
    "df['GeoBoxId'] = zeList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of box densities\n",
    "df_boxdensity = df.groupby('GeoBoxId').count()\n",
    "df_boxdensity = df_boxdensity[['name']]\n",
    "df_boxdensity.columns = ['count']\n",
    "df_boxdensity['density'] = df_boxdensity['count']/Box_Area\n",
    "df_boxdensity.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of hotel density - with square boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a map centered on Paris\n",
    "mymap = folium.Map(location=[LAT_PARIS,LON_PARIS], zoom_start=ZOOM_START_PARIS, tiles=TILE_PARIS)\n",
    "    \n",
    "folium.Choropleth(\n",
    "    geo_data = boundaries_box\n",
    "    ,data=df_boxdensity\n",
    "    ,columns=[df_boxdensity.index, 'count']\n",
    "    ,key_on='properties.objectid'\n",
    "    ,fill_color='YlOrRd'\n",
    "    ,fill_opacity=0.5 \n",
    "    ,line_opacity=0.2\n",
    "    ,legend_name='Hotels per square'\n",
    ").add_to(mymap)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proceed_with_map(mymap,'datasaved/Map_Paris_DensityBoxes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of hotel density - with Arrondissements (Zip Code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the zipcode boundaries as a geo json file from the city open data website   \n",
    "https://opendata.paris.fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZeGeoStr = project.get_file('Paris_arrondissements.geojson').read().decode('utf-8')\n",
    "#boundaries_arr = json.loads(ZeGeoStr)\n",
    "\n",
    "f = open('datasources/Paris_arrondissements.geojson', 'r')\n",
    "myGeoJsonFile = f.read()\n",
    "f.close()\n",
    "boundaries_arr = json.loads(myGeoJsonFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map of hotel density per arrondissement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the hotel don't have a postal code.  \n",
    "Some of the postal code may be wrong (user's data)  \n",
    "So we use google geocoding api to get the postal_code, and so, the Arrondissement, which is the last two digits of the postal\n",
    "code  \n",
    "  \n",
    "Alternatively, and more efficiently, we could have used the geo json file from opendata.paris.fr and a lib function to test if the hotel coords are in the polygon. Working like that, I could have gotten the postal code without sending any Geocode API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_Zip = []\n",
    "List_Arr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA : \n",
    "    print('NOT SENDING API CALLS, LOADING SAVED DATA INSTEAD')\n",
    "    raise StopExecution\n",
    "\n",
    "nbOk = len(List_Zip)\n",
    "for index, row in df.iterrows() :\n",
    "    if index >= nbOk :\n",
    "        ZipCode=''\n",
    "        latlon = str(row['lat']) + ',' + str(row['lon'])\n",
    "        url = 'https://maps.googleapis.com/maps/api/geocode/json?latlng={}&key={}&sensor=True&result_type=postal_code'.format(latlon,GOOGLE_API_KEY_GEOCODING)\n",
    "        results = requests.get(url).json()\n",
    "\n",
    "        try :\n",
    "            ZipCode = results['results'][0]['address_components'][0]['long_name']\n",
    "        except KeyError :\n",
    "            print('ERROR')\n",
    "            print(url)\n",
    "            print(result)\n",
    "\n",
    "        print('\\r','Running Geocoding Google Api request n°{}/{}  -  {}'.format(index,nbHotel,ZipCode),end='                           ')        \n",
    "        if len(ZipCode) == 5 :\n",
    "            List_Zip.append(str(ZipCode))\n",
    "            if ZipCode[0:2] == '75' :\n",
    "                List_Arr.append(int(ZipCode[3:5]))\n",
    "            else :\n",
    "                List_Arr.append(0)\n",
    "        else :\n",
    "            List_Zip.append('')\n",
    "            List_Arr.append(0)\n",
    "            print('Error for line n°{}  -  {}'.format(index,row['name']))\n",
    "\n",
    "df['ZipCodeTrue'] = List_Zip\n",
    "df['Arr'] = List_Arr\n",
    "print('\\r','Ready...',end='                           ')        \n",
    "\n",
    "# correct an error\n",
    "df.loc[472,'ZipCodeTrue']='75015'\n",
    "df.loc[472,'Arr']=15\n",
    "\n",
    "# save the work\n",
    "project.save_data(data=df.to_csv(index=False),file_name='datasaved/All01_arr.csv',overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA :\n",
    "    df = pd.DataFrame()\n",
    "    df = myLoad('datasaved/All01_arr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the density for each arrondissement    \n",
    "First get the surface from the geo json file  \n",
    "Then, calculate the density : nb of hotel / surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the areas of Arrondissement into a dataframe \n",
    "# (We are reading the previously used, paris opendata official, geojson file)\n",
    "zeList=[]\n",
    "for i in range(len(boundaries_arr['features'])) :\n",
    "    zeList.append([boundaries_arr['features'][i]['properties']['objectid'], boundaries_arr['features'][i]['properties']['surface']])\n",
    "df_arr_surface = pd.DataFrame(zeList)\n",
    "df_arr_surface.columns = ['Arr','surface']\n",
    "\n",
    "# Get number of hotel for each arrondissement\n",
    "df_Arr = df.groupby('Arr').count()\n",
    "df_Arr = df_Arr[['id']]\n",
    "df_Arr.columns=['nb']\n",
    "\n",
    "# merge the 2 dataframes\n",
    "df_Arr2 = pd.merge(df_Arr,df_arr_surface,on='Arr')\n",
    "\n",
    "# compute the density\n",
    "df_Arr2['density'] = df_Arr2['nb'] / df_Arr2['surface'] * 1000000\n",
    "\n",
    "df_Arr2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a map centered on Paris\n",
    "mymap = folium.Map(location=[LAT_PARIS,LON_PARIS], zoom_start=ZOOM_START_PARIS, tiles=TILE_PARIS)\n",
    "\n",
    "# use choropleth to draw arrondissement boundaries and associate a metric with the arrondissements\n",
    "folium.Choropleth(\n",
    "     geo_data=boundaries_arr\n",
    "    ,data=df_Arr2\n",
    "    ,columns=['Arr', 'density']\n",
    "    ,key_on='properties.objectid'\n",
    "    ,fill_color='YlOrRd'\n",
    "    ,fill_opacity=0.7 \n",
    "    ,line_opacity=0.2\n",
    "    ,legend_name='Hotel Density'\n",
    ").add_to(mymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proceed_with_map(mymap,'datasaved/Map_Paris_DensityArr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some context - Land prices & evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got Current Land prices & evolution on a website \n",
    "# and stored the data in a text file\n",
    "\n",
    "# HERE we  read the textfile and create dataframe\n",
    "with open('datasources/Prix_Paris.txt', 'r') as file:\n",
    "    myText = file.read().replace('\\n', '')\n",
    "\n",
    "# Let's parse it\n",
    "PrixParis = np.zeros((20,4))\n",
    "pos2=0\n",
    "for i in range(22) :\n",
    "    pos1 = pos2\n",
    "    pos2 = myText.find('</a><h2>', pos2+1) \n",
    "    #print('i {}  --  pos1 {} -- pos2 {}',i,pos1,pos2)\n",
    "    if i > 1 :\n",
    "        myExtract = myText[pos1:pos2]\n",
    "        #print('myextract : ' + myExtract)\n",
    "        #info1 = myExtract[myExtract.find('</h2>')+5:myExtract.find('€')-1]\n",
    "        info1 = myExtract[myExtract.find('</h2>')+5:myExtract.find('â‚¬')-1]\n",
    "        pos3 = myExtract.find('%')\n",
    "        info2 = myExtract[pos3+7:myExtract.find('<',pos3+4)-2]\n",
    "        pos4 = myExtract.find('/>', pos3+7)\n",
    "        info3 = myExtract[pos4+2:myExtract.find('<',pos4+4)-2]\n",
    "        #print('info1 =\"' + info1 + '\"')        \n",
    "        #print('info2 =\"' + info2 + '\"')\n",
    "        #print('info3 =\"' + info3 + '\"')\n",
    "        PrixParis[i-2,:]=[int(i-1),float(info1.replace(' ','')),float(info2.replace(' ','').replace(',','.')),float(info3.replace(' ','').replace(',','.'))]\n",
    "PrixParis[0,:]=[1,12660,11.2,25]  \n",
    "\n",
    "# Let's put it into a DataFrame\n",
    "df_price = pd.DataFrame(PrixParis)\n",
    "df_price.columns = ['Arr','Price','evol1y','evol5y']\n",
    "df_price['Arr'] = df_price['Arr'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymap1 = folium.Map(location=[LAT_PARIS,LON_PARIS], zoom_start=ZOOM_START_PARIS, tiles=TILE_PARIS)\n",
    "folium.Choropleth(\n",
    "     geo_data=boundaries_arr\n",
    "    ,data = df_price\n",
    "    ,columns=['Arr', 'Price']\n",
    "    ,key_on='properties.objectid'\n",
    "    ,fill_color='YlOrRd'\n",
    "    ,fill_opacity=0.7 \n",
    "    ,line_opacity=0.2\n",
    "    ,legend_name='Price of land'\n",
    ").add_to(mymap1)\n",
    "\n",
    "mymap2 = folium.Map(location=[LAT_PARIS,LON_PARIS], zoom_start=ZOOM_START_PARIS, tiles=TILE_PARIS)\n",
    "folium.Choropleth(\n",
    "     geo_data=boundaries_arr\n",
    "    ,data = df_price\n",
    "    ,columns=['Arr', 'evol5y']\n",
    "    ,key_on='properties.objectid'\n",
    "    ,fill_color='YlOrRd'\n",
    "    ,fill_opacity=0.7 \n",
    "    ,line_opacity=0.2\n",
    "    ,legend_name='Evolution for last 5 years of Price of land'\n",
    ").add_to(mymap2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Proceed_with_map(mymap1,'datasaved/Map_Paris_Arrondissement_Prices',DoShow=False)\n",
    "Proceed_with_map(mymap2,'datasaved/Map_Paris_Arrondissement_Prices_evol',DoShow=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td>\n",
    "        Current Land Prices\n",
    "    </td><td>\n",
    "        Evolution last 5 years\n",
    "    </td></tr>\n",
    "    <tr><td>\n",
    "    <img src='datasaved/Map_Paris_Arrondissement_Prices.png'></img>\n",
    "    </td><td>\n",
    "    <img src='datasaved/Map_Paris_Arrondissement_Prices_evol.png'></img>\n",
    "    </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr><td>\n",
    "        Hotel count per box\n",
    "    </td><td>\n",
    "        Hotel density per zip code\n",
    "    </td></tr>\n",
    "    <tr><td>\n",
    "    <img src='datasaved/Map_Paris_DensityBoxes.png'></img>\n",
    "    </td><td>\n",
    "    <img src='datasaved/Map_Paris_DensityArr.png'></img>\n",
    "    </td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have now a list of more than 2000 hotels in Paris' area.   \n",
    "### Let's get some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From 4square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- count of likes from 4square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "### This function returns the number of likes from 4square ###\n",
    "##############################################################\n",
    "# ex : get4squareLikeCount('4caa25212776b60ca80a450d')\n",
    "\n",
    "def get4squareLikeCount(VENUE_ID):\n",
    "    \n",
    "    # create the API request URL\n",
    "    url = 'https://api.foursquare.com/v2/venues/{}/likes?client_id={}&client_secret={}&v={}&limit={}'.format(\n",
    "        VENUE_ID, \n",
    "        SQUARE_CLIENT_ID, \n",
    "        SQUARE_CLIENT_SECRET, \n",
    "        VERSION, \n",
    "        100)\n",
    "    \n",
    "    results = requests.get(url).json()\n",
    "    \n",
    "    IsOk = results['meta']['code']\n",
    "    if IsOk != 200 :\n",
    "        print('')\n",
    "        print(\"ERROR code {} - Reponse : {}\".format(results['meta']['code'],results['meta']['errorDetail']))\n",
    "        print('html response : {}'.format(results))\n",
    "        print(url)\n",
    "        return('')\n",
    "\n",
    "    try :\n",
    "        nbLike = results[\"response\"]['likes']['count']\n",
    "    except KeyError :\n",
    "        nbLike = ''\n",
    "    \n",
    "    return(nbLike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA : \n",
    "    print('NOT SENDING API CALLS, LOADING SAVED DATA INSTEAD')\n",
    "    raise StopExecution\n",
    "\n",
    "# We could use this quick sentence to do it, but it takes more than 10min and I like to see the progress going on\n",
    "#             df['nblike'] = df.apply (lambda row: get4squareLikeCount(row.id), axis=1)\n",
    "\n",
    "# So let's use a row by row approach, \n",
    "# it will allow us to continue instead of restarting from zero if something happens\n",
    "# and it will show the ongoing work thanks to the 'print' function\n",
    "zeList=[]\n",
    "for index, row in df.iterrows :\n",
    "    zeList.append(get4squareLikeCount(row.id))\n",
    "    print('\\r','n°{} - {}'.format(index, row['name']),end='                    ')\n",
    "print('\\r','Ready...',end='                    ')\n",
    "df['nblike'] = zeList\n",
    "mySave(df,'datasaved\\All02_nbLike.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA :\n",
    "    df = pd.DataFrame()\n",
    "    df = myLoad('datasaved\\All02_nbLike.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to find their google ID   \n",
    "So that we can then extract their rating from google  \n",
    "<br>\n",
    "To get a match, we will use the name, longitude and latitude gotten with 4square to search for them on google in a 50m radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need first to create a google API key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGooglePlaceId(zePlaceName, zelat, zelon, zeradius) :\n",
    "    # ex GetGooglePlaceId('Ibis Budget',48.824220,2.260522,100)\n",
    "    \n",
    "    url = 'https://maps.googleapis.com/maps/api/place/findplacefromtext/json?input={}&inputtype=textquery&locationbias=circle:{}@{},{}&key={}'.format(\n",
    "        zePlaceName,\n",
    "        zeradius,\n",
    "        zelat,\n",
    "        zelon,\n",
    "        GOOGLE_API_KEY)\n",
    "    results = requests.get(url).json()\n",
    "    \n",
    "    try :\n",
    "        zeID = results['candidates'][0]['place_id']\n",
    "    except (KeyError,IndexError) :\n",
    "        zeID = ''\n",
    "\n",
    "    return(zeID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will run circa 2000 API calls to Google to get the id\n",
    "# May give an error from google who doesn't like, it seems, too many of my api calls ^^\n",
    "# So first instantiate the list here, to be able to re-run the next cell and continue filling the list\n",
    "zeID = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA : \n",
    "    print('NOT SENDING API CALLS, LOADING SAVED DATA INSTEAD')\n",
    "    raise StopExecution\n",
    "\n",
    "#################################################################################\n",
    "## Run Again if it stops with google closing down connection\n",
    "#################################################################################\n",
    "#df_allhotel.drop(columns=['google_id'], inplace=True)\n",
    "# Loops on the dataframe and run a query to get the google id on each row\n",
    "nbOk = len(zeID)\n",
    "for index, row in df.iterrows():\n",
    "    if index >= nbOk :\n",
    "        print('\\r', 'Running API call n°{}/{}   {}'.format(index,nbHotel,row['name']), end='                              ')\n",
    "        zeID.append(GetGooglePlaceId(row['name'],row['lat'],row['lon'],50))\n",
    "zeID\n",
    "df['google_id']=zeID\n",
    "print('\\r', 'Ready...', end='                              ')\n",
    "mySave(df,'datasaved\\All03_google_id.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA :\n",
    "    df = pd.DataFrame()\n",
    "    df = myLoad('datasaved\\All03_google_id.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the google id, let's get the google info, including the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a google place's id\n",
    "# make an API call to get this place's details\n",
    "# returns these details in a list\n",
    "\n",
    "def GetDetailsFromGoogleId(zeplaceid) :\n",
    "    # ex : GetDetailsFromGoogleId('ChIJFSBjnt9t5kcRb33P22PJ-RE')\n",
    "    # ex : https://maps.googleapis.com/maps/api/place/details/json?placeid=ChIJFSBjnt9t5kcRb33P22PJ-RE&key=AIzaSyDgibZm5ixDJv9g197Wpnhy9ZP6BArd8mc\n",
    "    \n",
    "    url = 'https://maps.googleapis.com/maps/api/place/details/json?placeid={}&key={}'.format(zeplaceid,GOOGLE_API_KEY)\n",
    "    results = requests.get(url).json()\n",
    "    #with open('GooglePlaceDetails.json', 'w') as f: {json.dump(results, f)}\n",
    "    myInfo = []\n",
    "    \n",
    "    try :\n",
    "        zeName = results['result']['name']\n",
    "    except KeyError :\n",
    "        zeName = ''\n",
    "    \n",
    "    try :\n",
    "        zeRating = results['result']['rating']\n",
    "    except KeyError :\n",
    "        zeRating =''\n",
    "\n",
    "    try :\n",
    "        zenbRating = results['result']['user_ratings_total']\n",
    "    except KeyError :\n",
    "        zenbRating =''\n",
    "        \n",
    "    try :\n",
    "        zeAddress = results['result']['formatted_address']\n",
    "    except KeyError :\n",
    "        zeAddress =''\n",
    "        \n",
    "    try :\n",
    "        zeLat = results['result']['geometry']['location']['lat']\n",
    "    except KeyError :\n",
    "        zeLat =''\n",
    "        \n",
    "    try :\n",
    "        zeLon = results['result']['geometry']['location']['lng']\n",
    "    except KeyError :\n",
    "        zeLon =''\n",
    "        \n",
    "    try :\n",
    "        zeType = results['result']['types']\n",
    "    except KeyError :\n",
    "        zeType =''\n",
    "            \n",
    "    myInfo = [zeName, zeRating, zenbRating, zeAddress, zeType, zeLat, zeLon]\n",
    "    return(myInfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the new columns of details to our main dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate here in case the next cell doesn't go all the way,  \n",
    "# so we can reload without loosing what we alreadygot from the api calls\n",
    "myList=[]\n",
    "myListDistance=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA : \n",
    "    print('NOT SENDING API CALLS, LOADING SAVED DATA INSTEAD')\n",
    "    raise StopExecution\n",
    "\n",
    "# Here we scan the dataframe to add google details for each row\n",
    "# As we do it, we also compute the distance between 4square coordinates and google coordinates\n",
    "# So that later on, We may delete every hotel with location different between 4square and google, that are bound to be matching errors\n",
    "\n",
    "nbOk = len(myList)\n",
    "for index, row in df.iterrows() :\n",
    "    if index >= nbOk :\n",
    "        if len(str(row['google_id'])) > 2 :\n",
    "            print('\\r', 'Running API call n°{}/{}   {}'.format(index,nbHotel,row['name']), end='                              ')\n",
    "            myList.append(GetDetailsFromGoogleId(row.google_id))\n",
    "            coords1 = (row.lat,row.lon)\n",
    "            coords2 = (myList[index][5],myList[index][6])\n",
    "            zeDist = geopy.distance.distance(coords1, coords2).km # distance between 4square & google positions\n",
    "            myListDistance.append(zeDist)\n",
    "        else :\n",
    "            myList.append([''])\n",
    "            myListDistance.append([''])\n",
    "\n",
    "df3 = pd.DataFrame(myList, columns=['google_name','google_rating','google_nbrating','google_address','google_type','google_lat','google_lon'])\n",
    "df4 = pd.DataFrame(myListDistance, columns=['distance'])\n",
    "df = pd.concat([df, df3], axis=1)\n",
    "df = pd.concat([df, df4], axis=1)\n",
    "\n",
    "mySave(df,'datasaved\\All04_google_details.csv')\n",
    "print('\\r', 'Ready...', end='                              ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA :\n",
    "    df = pd.DataFrame()\n",
    "    df = myLoad('datasaved\\All04_google_details.csv')\n",
    "    df = df.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : A more serious work may need some more time here.  \n",
    "To refine our 'google id finding process',   \n",
    "by, for example, trying other ways of matching (hotel's phone number or website url).  \n",
    "\n",
    "Spending more time on the subject, instead of dropping rows that have incorrect data, we could surely get better data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop hotels with no google id    \n",
    "Reason : They may be new or not well known  \n",
    "In many cases, it means data may not be relevant...  (or our matching sucks ^^)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} hostels listed'.format(df.shape[0]))\n",
    "df = df[df.google_id != '']\n",
    "#df = df[df.google_id.notnull()] # if reading from CSV\n",
    "print('But only {} with google id'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop hotels with wrong match    \n",
    "If the distance between 4square hotel position and google hotel position is greater than 50 meters,   \n",
    "- Maybe google didn't find the place inside the radius and so, try outside (ex Taj Mahal => India)  \n",
    "- Or the location is not precise enough to derive insight about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.distance < '0.05']\n",
    "print('Only {} with correct google location'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.duplicated(subset='google_id', keep=False) #'last', False\n",
    "ids = df['google_id']\n",
    "df = df[~ids.isin(ids[ids.duplicated()])]\n",
    "print('Only {} left'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop hotels with no google rating    \n",
    "They may be new or not well known, that is  to say, not enough data...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.google_rating != '']\n",
    "print('Only {} with scoring'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop hotels with too few google rating    \n",
    "data not statistically relevant...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.google_nbrating > 50]\n",
    "print('Only {} with more than 50 ratings'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop hotels that may not be in Paris\n",
    "We look at the city returned by google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['google_address'].str.contains(\"Paris, France\")]\n",
    "print('Only {} left'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop hotels that are not hotels    \n",
    "We use the 4square category for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#df.groupby('type').count()\n",
    "ListCatOk = ['Bed & Breakfast','Hostel','Hotel']\n",
    "df = df[df.type.isin(ListCatOk)]\n",
    "print('Only {} left'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySave(df,'datasaved\\All05_after_drop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = myLoad('datasaved\\All05_after_drop.csv')\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's have a look on a map to see the distribution over Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function create a gradient between 2 colors\n",
    "# We will use it on the map to colorize hotel markers according to their ratings\n",
    "def colorFader(c1,c2,mix=0): \n",
    "    c1=np.array(mpl.colors.to_rgb(c1))\n",
    "    c2=np.array(mpl.colors.to_rgb(c2))\n",
    "    return mpl.colors.to_hex((1-mix)*c1 + mix*c2)\n",
    "# ex : colorFader('#ff5050','#50ff50',0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instantiate map\n",
    "mymap = folium.Map(location=[LAT_PARIS,LON_PARIS], zoom_start=ZOOM_START_PARIS, tiles=TILE_PARIS) #tiles='Stamen Terrain', 'Stamen Toner'\n",
    "\n",
    "# populate map with hotel locations and some details\n",
    "Data_For_Marker = df[['name','google_rating','google_nbrating','lat','lon']].values.tolist()\n",
    "#Data_For_Marker = Data_For_Marker[0:100]\n",
    "nbMarker = len(Data_For_Marker)\n",
    "for i in range(nbMarker):\n",
    "    lat = Data_For_Marker[i][3]\n",
    "    lon = Data_For_Marker[i][4]\n",
    "    name = Data_For_Marker[i][0]\n",
    "    rating = Data_For_Marker[i][1]\n",
    "    nbrating = Data_For_Marker[i][2]\n",
    "    if rating < 3 :\n",
    "        zeColor = '#ff5050'\n",
    "    else :\n",
    "        zeColor = colorFader('#ff5050','#50ff50',(rating-3)/2)\n",
    "\n",
    "    #print('\\r', 'Drawing Marker n°{}/{}   {}'.format(i,nbMarker,name[0:10]), end='                              ')\n",
    "    #print('Drawing Marker n°{}/{}   {} - {} - {}'.format(i,nbMarker, rating, zeColor,name))\n",
    "    folium.CircleMarker(location = [lat, lon]\n",
    "                        ,radius = 5\n",
    "                        #,tooltip = str(name)\n",
    "                        ,popup = name.replace(\"'\",\"\") + '<br>Score : ' + str(rating) + '<br>Ratings qty : ' + str(int(nbrating))\n",
    "                        ,color = zeColor\n",
    "                        ,fillOpacity = 0.6\n",
    "                        , ).add_to(mymap)\n",
    "\n",
    "# ADD Train Stations\n",
    "for key in Dic_Station_Coords :\n",
    "    folium.CircleMarker(location = Dic_Station_Coords[key] ,radius = 5,popup = key.replace(\"'\",\"\\'\"),color = '#000000', ).add_to(mymap)\n",
    "\n",
    "# ADD Yellow Markers\n",
    "for key in Dic_Yellow_Coords :\n",
    "    folium.CircleMarker(location = Dic_Yellow_Coords[key] ,radius = 5,popup = key.replace(\"'\",\"\\'\"),color = '#ffff00', ).add_to(mymap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proceed_with_map(mymap,'datasaved/Map_Paris_Hotels_GoogleRating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see something directly.  \n",
    "There is a few area where hotels get better ratings.  \n",
    "You kind of find a new center for Paris...  \n",
    "It's not anymore <i>Notre-Dame</i>, the beautiful cathedral, who still stand today, but naked.  \n",
    "The new center may be put somewhere around <i>place de la Concorde</i>  \n",
    "I see also that all 5 train stations of Paris are on the opposite, sunked under a see of red spots.  \n",
    "I appreciate to get those insight, representing data, and, mixed with a bit of local knowledge, it's facinating.   \n",
    "So First insight : ratings are clearly geographically distributed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's draw another map of hotel distribution and associate colors with 4square count of likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['nblike'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of likes is widespread.   \n",
    "We will colorate our markers with the logartihm of number of like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# instantiate map\n",
    "mymap = folium.Map(location=[LAT_PARIS,LON_PARIS], zoom_start=ZOOM_START_PARIS, tiles=TILE_PARIS) #tiles='Stamen Terrain', 'Stamen Toner'\n",
    "\n",
    "# populate map with hotel locations and some details\n",
    "Data_For_Marker = df[['name','nblike','lat','lon']].values.tolist()\n",
    "nbMarker = len(Data_For_Marker)\n",
    "for i in range(nbMarker):\n",
    "    name = Data_For_Marker[i][0]\n",
    "    nblike = Data_For_Marker[i][1]\n",
    "    lat = Data_For_Marker[i][2]\n",
    "    lon = Data_For_Marker[i][3]\n",
    "    if nblike < 1 :\n",
    "        zeColor = '#ff5050'\n",
    "    #elif nblike > 100:\n",
    "    #    zeColor = '#50ff50'\n",
    "    else :\n",
    "        zeColor = colorFader('#ff5050','#50ff50',math.log(nblike)/math.log(737))\n",
    "\n",
    "    #print('\\r', 'Drawing Marker n°{}/{}   {}'.format(i,nbMarker,name[0:10]), end='                              ')\n",
    "    #print('Drawing Marker n°{}/{}   {} - {} - {}'.format(i,nbMarker, rating, zeColor,name))\n",
    "    folium.CircleMarker(location = [lat, lon]\n",
    "                        ,radius = 5\n",
    "                        #,tooltip = str(name)\n",
    "                        ,popup = name.replace(\"'\",\"\") + '<br>Score : ' + str(nblike)\n",
    "                        ,color = zeColor\n",
    "                        ,fillOpacity = 0.6\n",
    "                        , ).add_to(mymap)\n",
    "\n",
    "# ADD Train Stations\n",
    "for key in Dic_Station_Coords :\n",
    "    folium.CircleMarker(location = Dic_Station_Coords[key] ,radius = 5,popup = key.replace(\"'\",\"\\'\"),color = '#000000', ).add_to(mymap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4square likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proceed_with_map(mymap,'datasaved/Map_Paris_Hotels_4squareLikes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4square see things differently  \n",
    "But we look at a different kind of data  \n",
    "Here there is less green dots, but I used a logaritmic scale, due to the widespread nature of 'like count' data   \n",
    "the max of like is for places around the prestigious avenue <i>les champs élysés</i>, or another spot below with 2 hotels next to <i>la tour Effeil</i>    \n",
    "These neighboorhoods are high end, with prestigious companies having their office there.  \n",
    "  \n",
    "Something else : When you zoom out, you can see an outsider.   \n",
    "A bright green marker surounded by red ones on the far right hand side of Paris.   \n",
    "By clicking on it, you got his name : <i>Mama Shelter.  </i>\n",
    "I looked on the net. The concept is brillant and the company is growing fast.   \n",
    "Interesting. Still pricely, starting at 90€ a room.  \n",
    "It would be interesting to see where this growing company put its others hotels in other cities...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights about thoses 2 maps\n",
    "What we see is that there is 2/3 central regions, that have huge hotel density and good ratings.  \n",
    "In 4square, we see that the west gains the bigger number of Like.   \n",
    "Notably around the \"Champs Elysées\"  \n",
    "\n",
    "I didn't know about 4square before this course.  \n",
    "I think people using 4square are mainly tourists and foreign high-end workers.  \n",
    "\n",
    "That would not be a problem if our businessman is targeting this kind of people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df = myLoad('datasaved\\All05_after_drop.csv')\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if an unsupervised cluster algorithm will sort hotel into cluster that may match the 'local city center' seen on the distribution map of hotels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate hotels according to the most common venues found around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each hostel, we will look, like in our last project, at the nearby venues, using the 'venues/explore' 4square API  \n",
    "Then we will select the xth most common venues categories for each hotel, sorted by density. \n",
    "That allows us to create clusters, an unsupervised algorithm that will identify distinct type of neighboorhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform\n",
    "def getNearbyVenues(name, lat, lng, radius, limit):\n",
    "    global venues_list\n",
    "\n",
    "    # create the API request URL\n",
    "    url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "        SQUARE_CLIENT_ID, \n",
    "        SQUARE_CLIENT_SECRET, \n",
    "        VERSION, \n",
    "        lat, \n",
    "        lng, \n",
    "        radius, \n",
    "        limit)\n",
    "    #print(url)\n",
    "\n",
    "    # make the GET request\n",
    "    results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "    \n",
    "    # return only relevant information for each nearby venue\n",
    "    for v in range(len(results)) :\n",
    "        venues_list.append([\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            results[v]['venue']['name'], \n",
    "            results[v]['venue']['location']['lat'], \n",
    "            results[v]['venue']['location']['lng'],  \n",
    "            results[v]['venue']['categories'][0]['name']\n",
    "            ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_list=[]\n",
    "nbOk=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA : \n",
    "    print('NOT SENDING API CALLS, LOADING SAVED DATA INSTEAD')\n",
    "    raise StopExecution\n",
    "\n",
    "for index, row in df.iterrows() :\n",
    "    if index >= nbOk :\n",
    "        print('\\r', '{} - {}'.format(index,row['name']), end='                              ')\n",
    "        getNearbyVenues(row['name'],row['lat'],row['lon'],200,50)\n",
    "        nbOk = index + 1\n",
    "print('\\r', 'Ready...', end='                              ')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_DOWNLOAD_NEW_DATA : \n",
    "    nearby_venues = pd.DataFrame(venues_list)\n",
    "    nearby_venues.columns = ['Hostel', \n",
    "                  'Hostel_lat', \n",
    "                  'Hostel_lon', \n",
    "                  'Venue', \n",
    "                  'Venue_lat', \n",
    "                  'Venue_lon', \n",
    "                  'Venue_cat']  \n",
    "    mySave(nearby_venues,'datasaved\\Venues_by_Hotel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listvenues = myLoad('datasaved\\Venues_by_Hotel.csv')\n",
    "my_venues = df_listvenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print('There are {} uniques categories.'.format(len(my_venues['Venue_cat'].unique())))\n",
    "#my_venues.groupby('Hostel').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT ENCODING\n",
    "my_onehot = pd.get_dummies(my_venues[['Venue_cat']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add id column back to dataframe\n",
    "my_onehot['Hostel'] = my_venues['Hostel'] \n",
    "\n",
    "# move id column to the first column\n",
    "fixed_columns = [my_onehot.columns[-1]] + list(my_onehot.columns[:-1])\n",
    "my_onehot = my_onehot[fixed_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "my_grouped = my_onehot.groupby('Hostel').mean().reset_index()\n",
    "#my_grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create columns according to number of top venues\n",
    "columns = ['Hostel']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "hostel_venues_sorted = pd.DataFrame(columns=columns)\n",
    "hostel_venues_sorted['Hostel'] = my_grouped['Hostel']\n",
    "\n",
    "for ind in np.arange(my_grouped.shape[0]):\n",
    "    hostel_venues_sorted.iloc[ind, 1:] = return_most_common_venues(my_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "hostel_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# set number of clusters\n",
    "kclusters = kclusters_for_HostelVenues\n",
    "\n",
    "my_grouped_clustering = my_grouped.drop('Hostel', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(my_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%reset_selective -f my_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "hostel_venues_sorted.insert(0, 'cluster_labels', kmeans.labels_)\n",
    "\n",
    "my_merged = df\n",
    "\n",
    "# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
    "my_merged = my_merged.join(hostel_venues_sorted.set_index('Hostel'), on='name')\n",
    "\n",
    "#my_merged.head() # check the last columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column to store markers colors\n",
    "my_merged['marker_color'] = my_merged['cluster_labels'].apply(lambda x : colordict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySave(my_merged,'datasaved/All06_cluster.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map of the cluster colored hotels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate map\n",
    "mymap = folium.Map(location=[LAT_PARIS, LON_PARIS], zoom_start=ZOOM_START_PARIS, tiles=TILE_PARIS)\n",
    "\n",
    "# populate map with hotel locations and some details\n",
    "Data_For_Marker = my_merged[['name','cluster_labels','lat','lon','marker_color']].values.tolist()\n",
    "nbMarker = len(Data_For_Marker)\n",
    "for i in range(nbMarker):\n",
    "    name = Data_For_Marker[i][0]\n",
    "    myData = Data_For_Marker[i][1]\n",
    "    lat = Data_For_Marker[i][2]\n",
    "    lon = Data_For_Marker[i][3]\n",
    "    zeColor = Data_For_Marker[i][4]\n",
    "\n",
    "    folium.CircleMarker(location = [lat, lon]\n",
    "                        ,radius = 5\n",
    "                        #,tooltip = str(name)\n",
    "                        ,popup = name.replace(\"'\",\"\") + '<br>Cluster : ' + str(myData)\n",
    "                        ,color = zeColor\n",
    "                        ,fillOpacity = 0.6\n",
    "                        , ).add_to(mymap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proceed_with_map(mymap,'datasaved/Map_Paris_Clusters_Hotels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_merged['google_rating'] = pd.to_numeric(my_merged['google_rating'], errors='coerce')\n",
    "my_merged['google_nbrating'] = pd.to_numeric(my_merged['google_nbrating'], errors='coerce')\n",
    "my_merged.groupby('marker_color')['nblike','google_nbrating','google_rating'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure with which meaning to give to this ditribution.  \n",
    "\n",
    "One thing : red markers looks like 'active spots' with many shops.  \n",
    "Blue markers looks like they represent living neighborhoods (just appartement, not much shops)  \n",
    "\n",
    "Another thing : I can see that red spots are a majority in popular areas (east + north),   \n",
    "whereas blue spot are seens mainly in rich neighborhoods (west + south)  \n",
    "  \n",
    "These insight are kind of confirmed by the statistics shown by the groupby agregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another clustered map for comparaison\n",
    "Let's take something closer to what we last week in the IBM course :  \n",
    "Instead of using the hostels coordinates to get nearby venues,  \n",
    "we will create a map of neighborhoods and separate them into clusters   \n",
    "For this, we will use another set of geo json data, one with the administrative sectors of Paris.   \n",
    "(80 of them, 4 by arrondissement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get another geo json file for Paris neighboorhood\n",
    "f = open('datasources/quartier_paris.geojson', 'r')\n",
    "myGeoJsonFile = f.read()\n",
    "f.close()\n",
    "boundaries = json.loads(myGeoJsonFile)\n",
    "\n",
    "# Create a dictionnary of these neighborhood centers\n",
    "Dict_quartier = {}\n",
    "Dict_quartier.clear()\n",
    "for element in boundaries['features']:\n",
    "    Dict_quartier.update({element['properties']['l_qu'] : element['properties']['geom_x_y']})\n",
    "    \n",
    "# remove 5 centers that may become outsiders because in the middle of parks\n",
    "for x in ('Picpus','Bel-Air','Auteuil','Porte-Dauphine','Muette') :\n",
    "    Dict_quartier.pop(x,None)\n",
    "# removethis neighborhood which is also an outsider\n",
    "for x in ('Salpêtrière','toto'):\n",
    "    Dict_quartier.pop(x,None)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymap = folium.Map(location=[LAT_PARIS, LON_PARIS], zoom_start=ZOOM_START_PARIS, tiles=TILE_PARIS)\n",
    "\n",
    "# add polygons\n",
    "folium.GeoJson(boundaries,name='geojson').add_to(mymap)\n",
    "\n",
    "# add polygons centers\n",
    "for key in Dict_quartier :\n",
    "    folium.CircleMarker(location = Dict_quartier[key]\n",
    "                        ,radius = 5\n",
    "                        ,popup = key.replace(\"'\",\"\")\n",
    "                        ,fillOpacity = 0.6\n",
    "                        , ).add_to(mymap) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proceed_with_map(mymap,'datasaved/Map_Paris_Quartiers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_list=[]\n",
    "nbOk=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA : \n",
    "    print('NOT SENDING API CALLS, LOADING SAVED DATA INSTEAD')\n",
    "    raise StopExecution\n",
    "\n",
    "for key in Dict_quartier :\n",
    "    name = key.replace(\"'\",'')\n",
    "    print('\\r', '{}'.format(name), end='                              ')\n",
    "    getNearbyVenues(key,Dict_quartier[key][0],Dict_quartier[key][1],200,50)\n",
    "print('\\r', 'Ready...', end='                              ')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_DOWNLOAD_NEW_DATA : \n",
    "    nearby_venues = pd.DataFrame(venues_list)\n",
    "    nearby_venues.columns = ['id', \n",
    "                  'id_lat', \n",
    "                  'id_lon', \n",
    "                  'Venue', \n",
    "                  'Venue_lat', \n",
    "                  'Venue_lon', \n",
    "                  'Venue_cat']  \n",
    "    mySave(nearby_venues,'datasaved\\Venues_by_Quartier.csv')\n",
    "    my_venues = nearby_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA :\n",
    "    df = pd.DataFrame()\n",
    "    df_listvenues = myLoad('datasaved\\Venues_by_Quartier.csv')\n",
    "    my_venues = df_listvenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} uniques categories.'.format(len(my_venues['Venue_cat'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclusters = kclusters_for_quartiers80\n",
    "\n",
    "# ONE HOT ENCODING\n",
    "my_onehot = pd.get_dummies(my_venues[['Venue_cat']], prefix=\"\", prefix_sep=\"\")\n",
    "\n",
    "# add id column back to dataframe\n",
    "my_onehot['id'] = my_venues['id'] \n",
    "\n",
    "# move id column to the first column\n",
    "fixed_columns = [my_onehot.columns[-1]] + list(my_onehot.columns[:-1])\n",
    "my_onehot = my_onehot[fixed_columns]\n",
    "\n",
    "my_grouped = my_onehot.groupby('id').mean().reset_index()\n",
    "\n",
    "def return_most_common_venues(row, num_top_venues):\n",
    "    row_categories = row.iloc[1:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:num_top_venues]\n",
    "\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "\n",
    "# create columns according to number of top venues\n",
    "columns = ['id']\n",
    "for ind in np.arange(num_top_venues):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "venues_sorted = pd.DataFrame(columns=columns)\n",
    "venues_sorted['id'] = my_grouped['id']\n",
    "\n",
    "for ind in np.arange(my_grouped.shape[0]):\n",
    "    venues_sorted.iloc[ind, 1:] = return_most_common_venues(my_grouped.iloc[ind, :], num_top_venues)\n",
    "\n",
    "venues_sorted.head()\n",
    "\n",
    "my_grouped_clustering = my_grouped.drop('id', 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(my_grouped_clustering)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "#kmeans.labels_[0:10] \n",
    "\n",
    "# add clustering labels\n",
    "venues_sorted.insert(0, 'cluster_labels', kmeans.labels_)\n",
    "\n",
    "# create dataframe with neighboorhood id, lat & lon\n",
    "df1 = pd.DataFrame.from_dict(Dict_quartier, orient='index')\n",
    "df1.columns = ['lat','lon']\n",
    "my_merged = df1\n",
    "\n",
    "# merge id, lat, lon & closest venues\n",
    "my_merged = my_merged.join(venues_sorted.set_index('id'))\n",
    "\n",
    "# Correct empty Data    # dropna(). ??\n",
    "my_merged = my_merged.fillna('')\n",
    "my_merged = my_merged[my_merged.cluster_labels != '']\n",
    "my_merged['cluster_labels'].unique()\n",
    "\n",
    "# Cast as int\n",
    "my_merged['cluster_labels'] = my_merged['cluster_labels'].apply(lambda x : int(x))\n",
    "#my_merged.head(3)\n",
    "\n",
    "my_merged['marker_color'] = my_merged['cluster_labels'].apply(lambda x : colordict[x])\n",
    "\n",
    "mymap = folium.Map(location=[LAT_PARIS, LON_PARIS], zoom_start=ZOOM_START_PARIS, tiles=TILE_PARIS)\n",
    "\n",
    "for index, row in my_merged.iterrows():\n",
    "    folium.CircleMarker(location = [row['lat'],row['lon']]\n",
    "                        ,radius = 5\n",
    "                        ,popup = index\n",
    "                        ,color = row['marker_color']\n",
    "                        ,fillOpacity = 0.6\n",
    "                        , ).add_to(mymap)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Proceed_with_map(mymap,'datasaved/Map_Paris_Clusters_Quartiers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't give me any specific insight  \n",
    "If we set 3 or 4 clusters, I don't get anything visual as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Surely we could find a better neighborhoods geojson file, less administrative and more realistic.  \n",
    "* Also, another way would be to divide Paris in small boxes and run this venues search on each intersection point.  With enough points, that could give us something like a continuous map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, we could use our new cluster_label column as a feature below.\n",
    "I am not doing it but it is a TODO !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURES ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the density of restaurant, of sightseeing, or any other density may have an impact  \n",
    "I see a few things : \n",
    "- the existence : the customer can do it\n",
    "- the density : the more the merrier\n",
    "- the quality : the nec plus ultra (we won't do it but at the end of this report, you will find the functions to do it) \n",
    "\n",
    "We will use 4square to get all venues of a specific category and estimate presence and density  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = myLoad('datasaved\\All06_cluster.csv')\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetQuantities(lat, lon, categoryId, radius, limit) :\n",
    "#ex : GetQuantities(48.8485003,2.2662028,'4d4b7105d754a06374d81259', 200, 100)\n",
    "    VERSION='20190905'\n",
    "    \n",
    "    url = 'https://api.foursquare.com/v2/venues/search?intent=browse&client_id={}&client_secret={}&v={}&categoryId={}&ll={},{}&radius={}&limit={}'.format(\n",
    "        SQUARE_CLIENT_ID, \n",
    "        SQUARE_CLIENT_SECRET, \n",
    "        VERSION, \n",
    "        categoryId,\n",
    "        lat,\n",
    "        lon,\n",
    "        radius,\n",
    "        limit)\n",
    "\n",
    "    #print(url)\n",
    "    \n",
    "    results = requests.get(url).json()\n",
    "    #with open('list_venues.json', 'w') as f: {json.dump(results, f)}\n",
    "\n",
    "    IsOk = results['meta']['code']\n",
    "    if IsOk != 200 :\n",
    "        print('')\n",
    "        print(\"ERROR code {} - Reponse : {}\".format(results['meta']['code'],results['meta']['errorDetail']))\n",
    "        print(results)\n",
    "        print(url)\n",
    "        return([])\n",
    "    \n",
    "    try :\n",
    "        nbPlace = len(results['response']['venues'])    \n",
    "    except KeyError :\n",
    "        nbPlace = 0\n",
    "        return([])\n",
    "\n",
    "    myInfo=[]\n",
    "    for i in range (0,nbPlace) :\n",
    "        myInfo.append(results['response']['venues'][i]['location']['distance'])\n",
    "        \n",
    "    return(myInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnary of interesting 4square categories\n",
    "# for each intersting category, we associate the 4square id, the radius of research (for density) and a radius threshold (for close presence)\n",
    "\n",
    "CategoriesDic1 = {'food':['4d4b7105d754a06374d81259',100,30]\n",
    "                ,'Art_Entertainment': ['4d4b7104d754a06370d81259',300,30] #400 good\n",
    "                ,'Nightlife': ['4d4b7105d754a06376d81259',200,30]\n",
    "                ,'ATM':['52f2ab2ebcbc57f1066b8b56',800,400]\n",
    "                ,'Clothing_Store':['4bf58dd8d48988d103951735',200,100]\n",
    "                #,'Convenience_Store':'4d954b0ea243a5684a65b473'\n",
    "               }\n",
    "\n",
    "CategoriesDic2 = {'Metro_Station':['4bf58dd8d48988d1fd931735',200,100]\n",
    "                ,'Currency_Exchange':['5744ccdfe4b0c0459246b4be',300,100]\n",
    "                ,'Food_and_Drink_Shop' : ['4bf58dd8d48988d1f9941735',300,30]\n",
    "                ,'Wine_Shop' : ['4bf58dd8d48988d119951735',500,150]\n",
    "                ,'Monument_Landmark' : ['4bf58dd8d48988d12d941735',400,200]\n",
    "               }\n",
    "\n",
    "CategoriesAllDic = dict(CategoriesDic1, **CategoriesDic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoriesDic = CategoriesDic1\n",
    "myList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DO_DOWNLOAD_NEW_DATA : \n",
    "    print('NOT SENDING API CALLS, LOADING SAVED DATA INSTEAD')\n",
    "    raise StopExecution\n",
    "\n",
    "nbHotel = df.shape[0]\n",
    "nbOk = len(myList)\n",
    "nbRequest = 0\n",
    "for index, row in df.iterrows() :\n",
    "    if index >= nbOk :\n",
    "        print('\\r', 'nb api call : {} - Getting features - place n°{}/{} : {}'.format(nbRequest,index,nbHotel,row['name']), end='                              ')\n",
    "        myListDensity=[]\n",
    "        myListPresence=[]\n",
    "        myListDistance=[]\n",
    "\n",
    "# For each hotel, we run a series of api calls based on hotel coordinates\n",
    "        lat = row['lat']\n",
    "        lon = row['lon']\n",
    "        zeid = row['id'] \n",
    "        \n",
    "        for key in CategoriesDic :\n",
    "\n",
    "# For each category, we run an api call to return the list of nearby venues and their  distance to the hotel\n",
    "            category = key\n",
    "            categoryId = CategoriesDic[key][0]\n",
    "            radius = CategoriesDic[key][1]\n",
    "            presence_threshold = CategoriesDic[key][1]\n",
    "\n",
    "# Get distance list of this category venues  \n",
    "            ListDist = GetQuantities(lat, lon, categoryId, radius, limit=100)\n",
    "            nbDist = len(ListDist)\n",
    "            nbRequest += 1\n",
    "            \n",
    "# Calculate Presence\n",
    "            if nbDist == 0 :\n",
    "                pp = 0\n",
    "            else :\n",
    "                DistMinCount = np.sum(np.array(ListDist)<presence_threshold) # count nb of venues within radius 'presence_threshold'\n",
    "                if DistMinCount > 0 :\n",
    "                    pp = 1\n",
    "                else :\n",
    "                    pp = 0\n",
    "\n",
    "# Get density from number of venue\n",
    "            dd = nbDist / (3.14*((radius/1000)**2)) # nb/km²\n",
    "\n",
    "            myListDensity = myListDensity + [np.round(dd,1)]\n",
    "            myListPresence = myListPresence + [pp]\n",
    "            myListDistance = myListDistance + [ListDist]\n",
    "\n",
    "# Combine columns of results for this hotel            \n",
    "        myList.append([zeid] + myListDensity + myListPresence + myListDistance)\n",
    "\n",
    "# Construct Columns names for dataframe\n",
    "ColNames = [] \n",
    "ColNames.append('id')\n",
    "for key in CategoriesDic :\n",
    "    ColNames.append('d' + str(CategoriesDic[key][1]) + '_' + key)   # d200_Food\n",
    "for key in CategoriesDic :\n",
    "    ColNames.append('p' + str(CategoriesDic[key][2]) + '_' + key)   # p100_Food\n",
    "for key in CategoriesDic :\n",
    "    ColNames.append('l' + str(CategoriesDic[key][1]) + '_' + key)   # l100_Food\n",
    "\n",
    "# Save DataFrame to csv     \n",
    "df6 = pd.DataFrame(myList, columns=ColNames)\n",
    "file_name='CustomFeatures_' + str(df6.shape[0]) + '_' + str(int(time.time())) + '.csv'\n",
    "mySave(df6,'datasaved\\\\' + file_name)\n",
    "\n",
    "print('\\r', 'Ready...  {} API calls have been made'.format(nbRequest), end='                              ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_DOWNLOAD_NEW_DATA :\n",
    "    # add new columns to main dataframe\n",
    "    df5 = myLoad('datasaved\\CustomFeatures_957_1568386000.csv')\n",
    "    df = pd.merge(df,df5,on='id')\n",
    "    df6 = myLoad('datasaved\\CustomFeatures_957_1568633828.csv')\n",
    "    df = pd.merge(df,df6,on='id')\n",
    "    mySave(df,'datasaved\\All06_DicoDensity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance to a new center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = myLoad('datasaved\\All07_DicoDensity.csv')\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Transform the new center into a parameter \n",
    "# And find the new center that maximize r2 score\n",
    "\n",
    "LAT_NewCenter = 48.8646\n",
    "LON_NewCenter =  2.3214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_ToNewCenter(row):\n",
    "    return geopy.distance.distance([row['lat'],row['lon']],[LAT_NewCenter,LON_NewCenter]).m\n",
    "\n",
    "df['DistNewCenter'] = df.apply(calculate_distance_ToNewCenter, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"DistNewCenter\",y=\"google_rating\", data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a correlation between distance to New Center and google rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization of google rating to use it as target vector in Classification  Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Discrete_Google_Rating(google_rating) :\n",
    "    if google_rating >= THRESHOLD_GOOGLE_RATING :\n",
    "        return(1)\n",
    "    else :\n",
    "        return(0)\n",
    "df['google_rating_discrete'] = df['google_rating'].apply(lambda x : Get_Discrete_Google_Rating(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test set and normalizing data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# DATA\n",
    "X = df['DistNewCenter']\n",
    "y  = df['google_rating_discrete']\n",
    "y0  = df['google_rating']\n",
    "\n",
    "# TEST SET\n",
    "X_train0, X_test0, y_train0, y_test0 = train_test_split(X, y0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "X_train0 = np.array(X_train0).reshape(-1,1)\n",
    "X_test0 = np.array(X_test0).reshape(-1,1)\n",
    "\n",
    "#SCALER\n",
    "scaler = MinMaxScaler()\n",
    "#X_train = scaler.fit_transform(X_train)\n",
    "#X_test = scaler.transform(X_test)\n",
    "X_train = scaler.fit_transform(np.array(X_train).reshape(-1,1)) #only 1 features, need resize array\n",
    "X_test = scaler.transform(np.array(X_test).reshape(-1,1)) #only 1 features, need resize array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "print('Accuracy of K-NN classifier on training set: {:.2f}'\n",
    "     .format(knn.score(X_train, y_train)))\n",
    "print('Accuracy of K-NN classifier on test set: {:.2f}'\n",
    "     .format(knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm shows that, using just the hotel's coordinates,     \n",
    "we can guess the classification, that is to say : if its rating is above a threshold.  \n",
    "And this guess should be true around 70% of the time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's quantify the relation between this distance and continuous rating with linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : Try a polynomial of degree 2 or 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "plt.scatter(X_test0, y_test0,  color='black')\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train0, y_train0)\n",
    "y_pred0 = regr.predict(X_test0)\n",
    "plt.plot(X_test0, y_pred0, color='blue', linewidth=3)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n coef : {} \\n intercept : {}'.format(regr.coef_,regr.intercept_))\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test0, y_pred0))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test0, y_pred0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's say that the predictionn is far from perfect with a pretty low score around .1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The found coefficient is around -0.00015 and the found intercept is around 4.4.  \n",
    "That means that \n",
    "* if the hotel is 1000m from the center, his base rating would be 4.4 - 0.00015*1000 = 4.40 - 0.15 = 4.25\n",
    "* if the hotel is 4000m from the center, his base rating would be 4.4 - 0.00015*4000 = 4.40 - 0.60 = 3.80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correlation between created 'presence & density' features and 'google rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in CategoriesAllDic :\n",
    "    myFeatureDensity = 'd' + str(CategoriesAllDic[key][1]) + '_' + key\n",
    "    myFeaturePresence = 'p' + str(CategoriesAllDic[key][2]) + '_' + key\n",
    "\n",
    "    for i in (0,1) :\n",
    "        if i == 0 :\n",
    "            WhatIsTested = myFeatureDensity\n",
    "        else :\n",
    "            WhatIsTested = myFeaturePresence\n",
    "        \n",
    "        # DATA\n",
    "        X = df[WhatIsTested]\n",
    "        y  = df['google_rating_discrete']\n",
    "        y0  = df['google_rating']\n",
    "\n",
    "        # TEST SET\n",
    "        X_train0, X_test0, y_train0, y_test0 = train_test_split(X, y0)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "        X_train0 = np.array(X_train0).reshape(-1,1)\n",
    "        X_test0 = np.array(X_test0).reshape(-1,1)\n",
    "\n",
    "        plt.scatter(X_test0, y_test0,  color='black')\n",
    "        regr = linear_model.LinearRegression()\n",
    "        regr.fit(X_train0, y_train0)\n",
    "        y_pred0 = regr.predict(X_test0)\n",
    "        plt.plot(X_test0, y_pred0, color='blue', linewidth=3)\n",
    "\n",
    "        # RETURN RESULTS\n",
    "        print('DATA : {}'.format(WhatIsTested))\n",
    "        print('Coefficients: \\n coef : {} \\n intercept : {}'.format(regr.coef_,regr.intercept_))\n",
    "        print(\"Mean squared error: %.2f\" % mean_squared_error(y_test0, y_pred0))\n",
    "        print('Variance score: %.2f' % r2_score(y_test0, y_pred0))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What can we say :</b>\n",
    "\n",
    "Again, R2 score are very low, that is to say we can not predict the rating of the hotel with a simple feature like the restaurant density. That is normal !   \n",
    "\n",
    "Density and Presence don't give very different result and in the following study, we will only use density\n",
    " \n",
    "\n",
    "* Food - presence or density of food doesn't seem to have any real impact, maybe a smal negative correlation\n",
    "* Art & Entertainement (300m density) - POSITIVE correlation\n",
    "* NightLife (200m density) - POSITIVE correlation\n",
    "* ATM (800m density) - POSITIVE correlation\n",
    "* Clothing Store (200m density) - POSITIVE correlation\n",
    "* Metro Station - doen't seem to have any impact\n",
    "* Currency exchange (300m density) - POSITIVE correlation\n",
    "* Food & Drink Shops - doen't seem to have any impact, or a small negative one\n",
    "* Wine shop (500m density) - POSITIVE correlation\n",
    "* Monument_Landmark (400m density) - POSITIVE correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the correlation matrix for the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['google_rating'\n",
    "                 ,'d300_Art_Entertainment'\n",
    "                 ,'d200_Nightlife'\n",
    "                 ,'d800_ATM'\n",
    "                 ,'d200_Clothing_Store'\n",
    "                 ,'d300_Currency_Exchange'\n",
    "                 ,'d500_Wine_Shop'\n",
    "                 ,'d400_Monument_Landmark']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features looks highly correlated (that is normal since we use the same kind of geographic features.)    \n",
    "For exemple, <u>Art Entertainment</u> look very close to <u>Wine Shop</u> or to <u>Night Life</u>  \n",
    "So we will drop some features to avoid side effects of correlated features in algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, for selecting good features, we are comparing r2 score with different set of features  \n",
    "We run our simulation 20 times to get the means in between many different training/text set partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test set and normalizing data\n",
    "Dico_Test = {'d200_Nightlife':0.0\n",
    "             ,'d800_ATM':0.0\n",
    "             ,'d200_Clothing_Store':0.0\n",
    "             ,'d300_Currency_Exchange':0.0\n",
    "             ,'d500_Wine_Shop':0.0\n",
    "             ,'d400_Monument_Landmark':0.0} \n",
    "\n",
    "df0 = pd.DataFrame()\n",
    "df_score = pd.DataFrame()\n",
    "\n",
    "# DATA\n",
    "X = df[['DistNewCenter','d200_Nightlife','d800_ATM','d200_Clothing_Store','d300_Currency_Exchange','d500_Wine_Shop','d400_Monument_Landmark']]\n",
    "y  = df['google_rating_discrete']\n",
    "\n",
    "for i in range(20) :\n",
    "\n",
    "    # TEST SET\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    # We will compare r2 score with this reference feature\n",
    "    X_train_ref = X_train[['DistNewCenter']]\n",
    "    X_test_ref = X_test[['DistNewCenter']]\n",
    "    X_train_ref = scaler.fit_transform(np.array(X_train_ref).reshape(-1,1)) #only 1 features, need resize array\n",
    "    X_test_ref = scaler.transform(np.array(X_test_ref).reshape(-1,1)) #only 1 features, need resize array\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train_ref, y_train)\n",
    "    df0 = pd.DataFrame()\n",
    "    df0 = df0.append({'Ref' : knn.score(X_test_ref, y_test)}, ignore_index=True)\n",
    "\n",
    "    for key in Dico_Test :\n",
    "\n",
    "        # Limit to some features\n",
    "        X_train0 = X_train[['DistNewCenter',key]]\n",
    "        X_test0 = X_test[['DistNewCenter',key]]\n",
    "\n",
    "        # SCALER\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train0 = scaler.fit_transform(X_train0)\n",
    "        X_test0 = scaler.transform(X_test0)\n",
    "\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn.fit(X_train0, y_train)\n",
    "        Dico_Test[key] = knn.score(X_test0, y_test)\n",
    "\n",
    "    df_score_tmp = pd.DataFrame(Dico_Test.values())\n",
    "    df_score_tmp.columns = ['pass ' + str(i)]\n",
    "    df_score_tmp = pd.concat([df0, df_score_tmp], sort = False, axis = 0)\n",
    "    \n",
    "    df_score = pd.concat([df_score, df_score_tmp], sort = False, axis = 1)\n",
    "\n",
    "df_score.index = ['None','d200_Nightlife','d800_ATM','d200_Clothing_Store','d300_Currency_Exchange','d500_Wine_Shop','d400_Monument_Landmark']\n",
    "df_score = df_score.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_score.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While comparing to First 'None' column that represent the classification with only the NewDistance feature  \n",
    "We can see that adding <b>\n",
    "* Night Life  </b>\n",
    "\n",
    "actually decrease our score   \n",
    "These features may be to much corellated with the new distance feature  \n",
    "I will drop them \n",
    "\n",
    "On the other way <b>\n",
    "* Clothing Store\n",
    "* Currency Exchange\n",
    "* ATM  </b>\n",
    "\n",
    "Have a small positive impact\n",
    "\n",
    "While  <b>\n",
    "* Monument Landmark\n",
    "* Wine Shop     </b>\n",
    "\n",
    "have the best impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smaller set of features. Next step is to compare alorithms\n",
    "Here I should work more on the feature selection. Some combinaison are surely better that the one I use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Results container\n",
    "Dic_Result = {'KNN':[0,0],'LogReg':[0,0],'DecTree':[0,0]}\n",
    "\n",
    "# DATA\n",
    "X = df[['DistNewCenter','d800_ATM','d300_Currency_Exchange','d500_Wine_Shop','d400_Monument_Landmark']]\n",
    "y  = df['google_rating_discrete']\n",
    "\n",
    "# TEST SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# SCALER\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ALGOS:\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "Dic_Result['KNN'][0] = knn.score(X_train, y_train)\n",
    "Dic_Result['KNN'][1] = knn.score(X_test, y_test)\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "logreg.fit(X_train, y_train)\n",
    "Dic_Result['LogReg'][0] = logreg.score(X_train, y_train)\n",
    "Dic_Result['LogReg'][1] = logreg.score(X_test, y_test)\n",
    "\n",
    "# DECISION TREES\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "Dic_Result['DecTree'][0] = clf.score(X_train, y_train)\n",
    "Dic_Result['DecTree'][1] = clf.score(X_test, y_test)\n",
    "\n",
    "Dic_Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "90% of the time we run the simulation, we got the best score with the Logistic Regression  \n",
    "I sure should try other parameters (other K for KNN for exemple)  \n",
    "It's a TODO !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pred = logreg.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow up  -  To be continued ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of the hostels according to google rating of nearby venues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each hotel, we get a list of all venues of category X in a radius of Y meters  \n",
    "and extract the google rating, the price range and the distance to the hotel  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can build new qualitative features   \n",
    "For exemple :\n",
    "We get the list of all nearby restaurants, their distance, their rating, their price range.  \n",
    "We can create a metric by weighting ratings with distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hereunder, the code to get all venues from google\n",
    "with the usage of <b>next_page_token</b> to get the following results after 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetGoogleInfo(zetype,maxprice,radius,lat,lon) :\n",
    "\n",
    "    global next_page_token\n",
    "    next_page_token = ''\n",
    "    df = pd.DataFrame({'Name':[],'price':[],'rating':[],'nbRating':[],'lat':[],'lon':[]})\n",
    "\n",
    "    InfoGoogle = GetGoogleInfoPage(zetype,maxprice,radius,lat,lon)\n",
    "    dftmp = pd.DataFrame(InfoGoogle)\n",
    "    if dftmp.shape[0] != 0 :\n",
    "        dftmp.columns = ['Name','price','rating','nbRating','lat','lon']\n",
    "        df = df.append(dftmp)\n",
    "\n",
    "    while next_page_token != '' :\n",
    "        time.sleep(0.5) ## sinon il fait plusieurs fois le meme apel il me semble\n",
    "        InfoGoogle = GetGoogleInfoPage(zetype,maxprice,radius,lat,lon)\n",
    "        dftmp = pd.DataFrame(InfoGoogle)\n",
    "        if dftmp.shape[0] != 0 :\n",
    "            dftmp.columns = ['Name','price','rating','nbRating','lat','lon']\n",
    "            df = df.append(dftmp)\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(columns=['index'],inplace=True)\n",
    "\n",
    "    return(df)\n",
    "\n",
    "def GetGoogleInfoPage(zetype,maxprice,radius,lat,lon) :\n",
    "    global next_page_token\n",
    "\n",
    "    if next_page_token != '' :\n",
    "        url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json?key={}&pagetoken={}'.format(\n",
    "            GOOGLE_API_KEY,\n",
    "            next_page_token)\n",
    "    else :\n",
    "        url = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json?location={},{}&type={}&maxprice={}&radius={}&maxResults=100&key={}'.format(\n",
    "            lat,\n",
    "            lon,\n",
    "            zetype,\n",
    "            maxprice,\n",
    "            radius,\n",
    "            GOOGLE_API_KEY)\n",
    "\n",
    "    #print(url)\n",
    "    # make the GET request\n",
    "    results = requests.get(url).json()\n",
    "    nbPlace = len(results['results'])\n",
    "    InfoGoogle = []\n",
    "    if nbPlace > 0 :\n",
    "        # print('nb Place : {}   --   next token : {}'.format(nbPlace,next_page_token[1:10]))\n",
    "        try :\n",
    "            next_page_token = results['next_page_token']\n",
    "        except KeyError:\n",
    "            next_page_token = ''\n",
    "        for i in range (0,nbPlace) :\n",
    "            InfoGoogle.append([results['results'][i]['name'], \n",
    "                               results['results'][i]['price_level'], \n",
    "                               results['results'][i]['rating'], \n",
    "                               results['results'][i]['user_ratings_total'],\n",
    "                               results['results'][i]['geometry']['location']['lat'], \n",
    "                               results['results'][i]['geometry']['location']['lng']])\n",
    "\n",
    "    return(InfoGoogle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, we get, from google, ALL restaurant venues in 500m radius of one point\n",
    "\n",
    "# PARAMETERS\n",
    "################################################################\n",
    "lat=48.8534\n",
    "lon=2.3488\n",
    "radius=500\n",
    "zetype = 'restaurant'\n",
    "# restaurant, cafe, bakery, supermarket, convenience_store\n",
    "# subway_station, bus_station, train_station, atm\n",
    "# night_club, museum, travel_agency\n",
    "maxprice = 3\n",
    "\n",
    "# API CALLS (TODO : To check, maybe a problem sending the same api call multiple times)\n",
    "################################################################\n",
    "df_google_venues = GetGoogleInfo(zetype,maxprice,radius,lat,lon)\n",
    "print('{} places found'.format(df.shape[0]))\n",
    "\n",
    "# Add Distance Column \n",
    "################################################################\n",
    "def calculate_distance_ToHotel(row):\n",
    "    return geopy.distance.distance([row['lat'],row['lon']],[lat,lon]).m\n",
    "df_google_venues['dist'] = df_google_venues.apply(calculate_distance_ToHotel, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show count, grouped by google price bins\n",
    "df_google_venues.groupby(['price']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all, sorted by distance\n",
    "df_google_venues.sort_values(by=['dist'], ascending = True, inplace = True)\n",
    "df_google_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
